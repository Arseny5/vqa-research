{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aeb113ae-e347-4e66-abf7-2d2d57816770",
   "metadata": {},
   "source": [
    "# Fine-tuning ROBERTA + VIT for binary visual question answering (VQA)\n",
    "\n",
    "![ViT architecture](https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/transformers/model_doc/vit_architecture.jpg)\n",
    "\n",
    "* ViT paper: https://arxiv.org/pdf/2010.11929.pdf\n",
    "* ViT docs: https://huggingface.co/docs/transformers/model_doc/vit\n",
    "* ROBERTA paper: https://arxiv.org/pdf/1907.11692.pdf\n",
    "* ROBERTA docs: https://huggingface.co/docs/transformers/model_doc/roberta"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23c0ed8-8c7e-4100-9fe3-cb1aae2f8fba",
   "metadata": {},
   "source": [
    "## Set-up environment: Imports and Configurations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "998e2bc3-1489-4433-809e-27596a6fe4d2",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "3b535349-97c7-4043-9b65-b823065148c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -qqq easy-vqa\n",
    "# !pip install -qqq sentence_transformers transformers timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f47ec0-f45f-4a9f-a2b3-f42080e56a2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install wandb\n",
    "# !pip install -q evaluate rouge_score\n",
    "# !pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "id": "9f801c6c-c960-4bcd-aba1-02356d32144f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import warnings\n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import evaluate\n",
    "import requests\n",
    "import torch\n",
    "import random\n",
    "import argparse\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "from copy import deepcopy\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "from torchvision.io import read_image\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.utils.data import Dataset\n",
    "from transformers import ViTImageProcessor, ViTForImageClassification\n",
    "from transformers import AutoTokenizer, AutoFeatureExtractor, AutoModel, RobertaModel, RobertaTokenizer\n",
    "import torchvision.transforms as T\n",
    "from torchvision import transforms\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.metrics import accuracy_score\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torch.optim import AdamW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "id": "c76efd83-8730-4780-b34f-90a1e3abeecd",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = '/home/jovyan/ars/vqa-research/VQA/data/'\n",
    "ANNOTATIONS_PATH = DATA_PATH + 'abstract_v002_train2017_annotations.json'\n",
    "QUESTIONS_PATH = DATA_PATH + 'OpenEnded_abstract_v002_train2017_questions.json'\n",
    "VQA_TRAIN_PATH = DATA_PATH + 'VQA_train.csv'\n",
    "IMAGE_DIR = DATA_PATH + 'scene_img_abstract_v002_train2017/'\n",
    "IMAGE_PREFIX = 'abstract_v002_train2015_'\n",
    "IMAGE_FORMAT = '.png'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "id": "aaba7b6e-3f4e-4d85-9f45-7969b23f9c1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 1234\n",
    "warnings.filterwarnings('ignore')\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "id": "80286271-2acd-4e60-adeb-a9653065f781",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f'Device: {device}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca4fe19-7d51-44d0-93e8-47d597ad8762",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "id": "e32f90be-57dc-4d25-9316-7f0609e4f14b",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_data = pd.read_csv(VQA_PATH, index_col=0)\n",
    "\n",
    "with open(ANNOTATIONS_PATH) as f:\n",
    "    annotations = json.load(f)['annotations']\n",
    "\n",
    "with open(QUESTIONS_PATH) as f:\n",
    "    questions = json.load(f)['questions']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "id": "bb64d97f-0097-4f98-92c9-d41d1258f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_lowercase(vqa_data):\n",
    "    X = vqa_data.copy()\n",
    "    X[['question', 'answer']].apply(lambda x: x.str.lower(), axis=0)\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "id": "7627624c-0a9d-47f2-ae35-2a03c5eca18a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_data = to_lowercase(vqa_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "id": "1497d46b-56ad-4382-9522-6b0717648c31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_img_paths(vqa_data, img_dir, prefix, format='.png'):\n",
    "    X = vqa_data.copy()\n",
    "    X['image_path'] = \\\n",
    "        img_dir + \\\n",
    "        prefix + \\\n",
    "        (12 - vqa_data['image_id'].astype('str').str.len()).apply(lambda x: x * '0') + \\\n",
    "    vqa_data['image_id'].astype('str') + format\n",
    "    return X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "id": "976e2342-fe67-4de3-8dee-320dc39a5dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_data = add_img_paths(vqa_data, IMG_DIR, IMG_PREFIX, IMG_FORMAT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "id": "213c027c-5fb5-4232-b725-2fcb0983a47c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(22055, 5)"
      ]
     },
     "execution_count": 351,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "id": "bef23bd2-f3be-4241-ac1a-d794bec1c9eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "vqa_data['label'] = vqa_data['answer'].apply(lambda x: 1 if x == 'yes' else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "id": "991f3e14-1650-47f7-8fb5-7e42052437b2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>question</th>\n",
       "      <th>question_id</th>\n",
       "      <th>answer</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>87</td>\n",
       "      <td>Is the boy having wine?</td>\n",
       "      <td>870</td>\n",
       "      <td>yes</td>\n",
       "      <td>/home/jovyan/ars/vqa-research/VQA/data/scene_i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>900000870</td>\n",
       "      <td>Is the boy having wine?</td>\n",
       "      <td>900000870</td>\n",
       "      <td>no</td>\n",
       "      <td>/home/jovyan/ars/vqa-research/VQA/data/scene_i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>14962</td>\n",
       "      <td>Is it night time?</td>\n",
       "      <td>149620</td>\n",
       "      <td>no</td>\n",
       "      <td>/home/jovyan/ars/vqa-research/VQA/data/scene_i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>900149620</td>\n",
       "      <td>Is it night time?</td>\n",
       "      <td>900149620</td>\n",
       "      <td>yes</td>\n",
       "      <td>/home/jovyan/ars/vqa-research/VQA/data/scene_i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8277</td>\n",
       "      <td>Is the boy hanging from monkey bars?</td>\n",
       "      <td>82771</td>\n",
       "      <td>yes</td>\n",
       "      <td>/home/jovyan/ars/vqa-research/VQA/data/scene_i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    image_id                              question  question_id answer  \\\n",
       "0         87               Is the boy having wine?          870    yes   \n",
       "1  900000870               Is the boy having wine?    900000870     no   \n",
       "2      14962                     Is it night time?       149620     no   \n",
       "3  900149620                     Is it night time?    900149620    yes   \n",
       "4       8277  Is the boy hanging from monkey bars?        82771    yes   \n",
       "\n",
       "                                          image_path  label  \n",
       "0  /home/jovyan/ars/vqa-research/VQA/data/scene_i...      1  \n",
       "1  /home/jovyan/ars/vqa-research/VQA/data/scene_i...      0  \n",
       "2  /home/jovyan/ars/vqa-research/VQA/data/scene_i...      0  \n",
       "3  /home/jovyan/ars/vqa-research/VQA/data/scene_i...      1  \n",
       "4  /home/jovyan/ars/vqa-research/VQA/data/scene_i...      1  "
      ]
     },
     "execution_count": 353,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vqa_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "id": "6f0dda58-db0c-4094-8bda-051fa69ad953",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15438, 4), (3308, 4), (3309, 4))"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del vqa_data['image_id']\n",
    "del vqa_data['question_id']\n",
    "\n",
    "train_df, temp_data = train_test_split(vqa_data, test_size=0.3, random_state=42)\n",
    "test_df, eval_df = train_test_split(temp_data, test_size=0.5, random_state=42)\n",
    "\n",
    "train_df.shape, test_df.shape, eval_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "06fcf183-a3b7-4806-bbf3-c97b84ef80e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>question</th>\n",
       "      <th>answer</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Are they feeding the birds?</td>\n",
       "      <td>yes</td>\n",
       "      <td>/home/jovyan/ars/vqa-research/VQA/data/scene_i...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Is there a flower vase on the table?</td>\n",
       "      <td>no</td>\n",
       "      <td>/home/jovyan/ars/vqa-research/VQA/data/scene_i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Are there two bookcases?</td>\n",
       "      <td>no</td>\n",
       "      <td>/home/jovyan/ars/vqa-research/VQA/data/scene_i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Are both people wearing white?</td>\n",
       "      <td>no</td>\n",
       "      <td>/home/jovyan/ars/vqa-research/VQA/data/scene_i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Is there a pond?</td>\n",
       "      <td>no</td>\n",
       "      <td>/home/jovyan/ars/vqa-research/VQA/data/scene_i...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               question answer  \\\n",
       "0           Are they feeding the birds?    yes   \n",
       "1  Is there a flower vase on the table?     no   \n",
       "2              Are there two bookcases?     no   \n",
       "3        Are both people wearing white?     no   \n",
       "4                      Is there a pond?     no   \n",
       "\n",
       "                                          image_path  label  \n",
       "0  /home/jovyan/ars/vqa-research/VQA/data/scene_i...      1  \n",
       "1  /home/jovyan/ars/vqa-research/VQA/data/scene_i...      0  \n",
       "2  /home/jovyan/ars/vqa-research/VQA/data/scene_i...      0  \n",
       "3  /home/jovyan/ars/vqa-research/VQA/data/scene_i...      0  \n",
       "4  /home/jovyan/ars/vqa-research/VQA/data/scene_i...      0  "
      ]
     },
     "execution_count": 314,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df = test_df.reset_index()\n",
    "del test_df['index']\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1e6a41-6f3d-4acf-b915-991f81336f8c",
   "metadata": {},
   "source": [
    "## Load encoders for visual and text modalities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "id": "d7aaa4f7-a70f-495a-b085-3f1a191f6312",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaModel: ['lm_head.dense.weight', 'lm_head.bias', 'lm_head.layer_norm.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.bias']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaModel were not initialized from the model checkpoint at roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "### ROBERTA for text ###\n",
    "model_name = \"roberta-base\"\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_name)\n",
    "text_encoder = RobertaModel.from_pretrained(model_name)\n",
    "\n",
    "for p in text_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "### VIT for text ###\n",
    "image_processor = AutoFeatureExtractor.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "image_encoder = AutoModel.from_pretrained(\"google/vit-base-patch16-224-in21k\")\n",
    "\n",
    "for p in image_encoder.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "\n",
    "image_encoder.to(device)\n",
    "text_encoder.to(device)\n",
    "print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "913a1667-5939-423e-9228-d288cc51bd19",
   "metadata": {},
   "source": [
    "## Create Dataset with pairs text-images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "id": "3d1b5d8f-026b-404d-90e4-867d926cfb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaVitDataset(Dataset):\n",
    "\n",
    "    def __init__(self,df,\n",
    "                 image_encoder,\n",
    "                 text_encoder,\n",
    "                 image_processor,\n",
    "                 tokenizer,\n",
    "              ):\n",
    "        self.df = df\n",
    "        self.image_encoder = image_encoder\n",
    "        self.text_encoder = text_encoder\n",
    "        self.image_processor = image_processor\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        image_file = self.df[\"image_path\"][idx]\n",
    "        question = self.df['question'][idx]\n",
    "        image = Image.open(image_file).convert(\"RGB\")\n",
    "        label = self.df['label'][idx]\n",
    "\n",
    "        # image = resize_transform(image)\n",
    "        # image_inputs = T.ToTensor()(image).unsqueeze_(0)\n",
    "        # image_inputs = image_inputs.to(device)\n",
    "        # image_outputs = self.image_encoder(image_inputs)\n",
    "        # image_embedding = image_outputs[0]\n",
    "        # image_embedding = image_embedding.detach()\n",
    "        # print(\"Image emb\", image_embedding.shape)\n",
    "\n",
    "        image_inputs = self.image_processor(image, return_tensors=\"pt\")\n",
    "        image_inputs = {k:v.to(device) for k,v in image_inputs.items()}\n",
    "        image_outputs = self.image_encoder(**image_inputs)\n",
    "        image_embedding = image_outputs.pooler_output\n",
    "        image_embedding = image_embedding.view(-1)\n",
    "        image_embedding = image_embedding.detach()\n",
    "        # print(\"Image emb\", image_embedding.shape)\n",
    "\n",
    "        text_inputs = self.tokenizer(question, return_tensors=\"pt\")\n",
    "        text_inputs = {k:v.to(device) for k,v in text_inputs.items()}\n",
    "        text_outputs = self.text_encoder(**text_inputs)\n",
    "        text_embedding = text_outputs.pooler_output \n",
    "        text_embedding = text_embedding.view(-1)\n",
    "        text_embedding = text_embedding.detach()\n",
    "        # print(\"Text emb\", text_embedding.shape)\n",
    "\n",
    "        encoding={}\n",
    "        encoding[\"image_emb\"] = image_embedding\n",
    "        encoding[\"text_emb\"] = text_embedding\n",
    "        encoding[\"label\"] = torch.tensor(label)\n",
    "\n",
    "        return encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "id": "1ae904ad-c13e-4668-9e7d-c5ac4019b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.reset_index(drop=True, inplace=True)\n",
    "eval_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "train_dataset = RobertaVitDataset(\n",
    "                           df=train_df,\n",
    "                           image_encoder = image_encoder,\n",
    "                           text_encoder = text_encoder,\n",
    "                           tokenizer = tokenizer,\n",
    "                           image_processor = image_processor, \n",
    "                           )\n",
    "\n",
    "eval_dataset = RobertaVitDataset(\n",
    "                           df=eval_df,\n",
    "                           image_encoder = image_encoder,\n",
    "                           text_encoder = text_encoder,\n",
    "                           tokenizer = tokenizer,\n",
    "                           image_processor = image_processor,\n",
    "                          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "id": "cb636ca2-56bd-4cd0-93b3-34cb59c09233",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "eval_batch_size = 32\n",
    "dataloader_train = DataLoader(train_dataset,\n",
    "                              sampler=RandomSampler(train_dataset),\n",
    "                              batch_size=batch_size)\n",
    "dataloader_validation = DataLoader(eval_dataset,\n",
    "                                   sampler=SequentialSampler(eval_dataset),\n",
    "                                   batch_size=eval_batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "id": "6242e9eb-11cc-4738-a616-7608ee2f49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81d6ed72-4f22-45e7-89fa-f9d790ec4d85",
   "metadata": {},
   "source": [
    "## Initialize evaluate and train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 383,
   "id": "67aec742-b021-4104-bbd2-7e7bcca11a4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(dataloader_val):\n",
    "\n",
    "    model.eval()\n",
    "    loss_val_total = 0\n",
    "    predictions, true_vals, confidence = [], [], []\n",
    "\n",
    "    for batch in dataloader_val:\n",
    "\n",
    "        batch = tuple(b.to(device) for b in batch.values())\n",
    "\n",
    "        inputs = {'image_emb':  batch[0],'text_emb': batch[1]}\n",
    "        # print(\"PRINT IMG EMB\")\n",
    "        # print(batch[0].shape)\n",
    "        # print(batch[0])\n",
    "        # print(\"PRINT TEXT EMB\")\n",
    "        # print(batch[1].shape)\n",
    "        # print(batch[1])\n",
    "        # break\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "\n",
    "        labels =  batch[2]\n",
    "        loss = criterion(outputs.view(-1, 13), labels.view(-1))\n",
    "        loss_val_total += loss.item()\n",
    "        probs   = torch.max(outputs.softmax(dim=1), dim=-1)[0].detach().cpu().numpy()\n",
    "        outputs = outputs.argmax(-1)\n",
    "        logits = outputs.detach().cpu().numpy()\n",
    "        label_ids = labels.cpu().numpy()\n",
    "        predictions.append(logits)\n",
    "        true_vals.append(label_ids)\n",
    "        confidence.append(probs)\n",
    "\n",
    "    loss_val_avg = loss_val_total/len(dataloader_val)\n",
    "    predictions = np.concatenate(predictions, axis=0)\n",
    "    true_vals = np.concatenate(true_vals, axis=0)\n",
    "    confidence = np.concatenate(confidence, axis=0)\n",
    "\n",
    "    return loss_val_avg, predictions, true_vals, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15851960-c682-4d7b-946b-6cd0123bd4a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    log_hdr  = \"Epoch, train_loss, train_acc, val_loss, val_acc\"\n",
    "    train_f1s = []\n",
    "    val_f1s = []\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "    min_val_loss = -1\n",
    "    max_auc_score = 0\n",
    "    epochs_no_improve = 0\n",
    "    early_stopping_epoch = 3\n",
    "    early_stop = False\n",
    "\n",
    "    for epoch in tqdm(range(1, epochs+1)):\n",
    "        model.train()\n",
    "        loss_train_total = 0\n",
    "        train_predictions, train_true_vals = [], []\n",
    "\n",
    "        progress_bar = tqdm(dataloader_train, desc='Epoch {:1d}'.format(epoch), leave=False, disable=False)\n",
    "\n",
    "        for batch in progress_bar:\n",
    "            model.zero_grad()\n",
    "            batch = tuple(b.to(device) for b in batch.values())\n",
    "\n",
    "            inputs = {'image_emb':  batch[0],'text_emb': batch[1]}\n",
    "            labels =  batch[2]\n",
    "\n",
    "            outputs = model(**inputs)\n",
    "            loss = criterion(outputs.view(-1, 13), labels.view(-1))\n",
    "            loss_train_total += loss.item()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "\n",
    "            logits = outputs.argmax(-1)\n",
    "            logits = logits.detach().cpu().numpy()\n",
    "            label_ids = labels.cpu().numpy()\n",
    "            train_predictions.append(logits)\n",
    "            train_true_vals.append(label_ids)\n",
    "\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            progress_bar.set_postfix({'training_loss': '{:.3f}'.format(loss.item()/len(batch))})\n",
    "\n",
    "\n",
    "\n",
    "        train_predictions = np.concatenate(train_predictions, axis=0)\n",
    "        train_true_vals = np.concatenate(train_true_vals, axis=0)\n",
    "\n",
    "        tqdm.write(f'\\nEpoch {epoch}')\n",
    "        loss_train_avg = loss_train_total/len(dataloader_train)\n",
    "        tqdm.write(f'Training loss: {loss_train_avg}')\n",
    "        train_f1 = accuracy_score_func(train_predictions, train_true_vals)\n",
    "        tqdm.write(f'Train Acc: {train_f1}')\n",
    "\n",
    "        val_loss, predictions, true_vals,_ = evaluate(dataloader_validation)\n",
    "        val_f1 = accuracy_score_func(predictions, true_vals)\n",
    "        tqdm.write(f'Validation loss: {val_loss}')\n",
    "        tqdm.write(f'Val Acc: {val_f1}')\n",
    "\n",
    "        if val_f1 >= max_auc_score:\n",
    "            tqdm.write('\\nSaving best model')\n",
    "            torch.save(model.state_dict(), f'/home/jovyan/ars/vqa-research/VQA/ROBERTA_VIT_model/models/easyvqa_finetuned_epoch_{epoch}.model')\n",
    "            max_auc_score = val_f1\n",
    "\n",
    "        train_losses.append(loss_train_avg)\n",
    "        val_losses.append(val_loss)\n",
    "        train_f1s.append(train_f1)\n",
    "        val_f1s.append(val_f1)\n",
    "        log_str  = \"{}, {}, {}, {}, {}\".format(epoch, loss_train_avg, train_f1, val_loss, val_f1)\n",
    "\n",
    "        if min_val_loss < 0:\n",
    "            min_val_loss = val_loss\n",
    "        else:\n",
    "            if val_loss < min_val_loss:\n",
    "                min_val_loss = val_loss\n",
    "            else:\n",
    "                epochs_no_improve += 1\n",
    "                if epochs_no_improve >= early_stopping_epoch:\n",
    "                    early_stop = True\n",
    "                    break\n",
    "                else:\n",
    "                    continue\n",
    "\n",
    "\n",
    "    if early_stop:\n",
    "        print(\"Early Stopping activated at epoch -\", epoch )\n",
    "        print(\"Use the checkpoint at epoch - \", epoch - early_stopping_epoch)\n",
    "\n",
    "    return train_losses, val_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b845cf64-64f4-42c2-944e-9c35bd7ed252",
   "metadata": {},
   "source": [
    "## Our fusion network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ef5670f-36c5-48a1-82fe-f8c6194d8d06",
   "metadata": {},
   "source": [
    "### Get text-emb from roberta and img-emb from vit and concatenate isung linear layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 359,
   "id": "15dc077d-fdac-4b30-bbe1-7f64c10d22b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RobertaVitFusionNetwork(nn.Module):\n",
    "    def __init__(self, hyperparms=None):\n",
    "\n",
    "        super(RobertaVitFusionNetwork, self).__init__()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.vision_projection = nn.Linear(2048, 768)\n",
    "        self.text_projection = nn.Linear(512, 768)\n",
    "        self.fc1 = nn.Linear(768, 256)\n",
    "        self.bn1 = nn.BatchNorm1d(256)\n",
    "        self.classifier = nn.Linear(256, 13)\n",
    "        W = torch.Tensor(768, 768)\n",
    "        self.W = nn.Parameter(W)\n",
    "        self.relu_f = nn.ReLU()\n",
    "        nn.init.kaiming_uniform_(self.W, a=math.sqrt(5))\n",
    "\n",
    "    def forward(self, image_emb, text_emb):\n",
    "\n",
    "        x1 = image_emb\n",
    "        x1 = torch.nn.functional.normalize(x1, p=2, dim=1)\n",
    "        Xv = self.relu_f(self.vision_projection(x1))\n",
    "\n",
    "        x2 = text_emb\n",
    "        x2 = torch.nn.functional.normalize(x2, p=2, dim=1)\n",
    "        Xt = self.relu_f(self.text_projection(x2))\n",
    "\n",
    "        Xvt = Xv * Xt\n",
    "        Xvt = self.relu_f(torch.mm(Xvt, self.W.t()))\n",
    "\n",
    "        Xvt = self.fc1(Xvt)\n",
    "        Xvt = self.bn1(Xvt)\n",
    "        Xvt = self.dropout(Xvt)\n",
    "        Xvt = self.classifier(Xvt)\n",
    "\n",
    "        return Xvt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 360,
   "id": "e4c5dab7-5346-4c83-817a-a38cc51e21e5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RobertaVitFusionNetwork(\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (vision_projection): Linear(in_features=2048, out_features=768, bias=True)\n",
       "  (text_projection): Linear(in_features=512, out_features=768, bias=True)\n",
       "  (fc1): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (classifier): Linear(in_features=256, out_features=13, bias=True)\n",
       "  (relu_f): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 360,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch.cuda.empty_cache()\n",
    "model = RobertaVitFusionNetwork()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "778f287d-cf0c-4dce-ae87-a1b8c6f33054",
   "metadata": {},
   "source": [
    "## Create optimizer and scheduler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "id": "b96f77a3-b33b-462a-b05c-03992d56a34f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_steps 20000\n",
      "warm_steps 2000.0\n"
     ]
    }
   ],
   "source": [
    "optimizer = AdamW(model.parameters(),\n",
    "                  lr=5e-5,\n",
    "                  weight_decay = 1e-5,\n",
    "                  eps=1e-8\n",
    "                  )\n",
    "\n",
    "epochs = 10\n",
    "train_steps=20000\n",
    "print(\"train_steps\", train_steps)\n",
    "warm_steps = train_steps * 0.1\n",
    "print(\"warm_steps\", warm_steps)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer,\n",
    "                                            num_warmup_steps=warm_steps,\n",
    "                                            num_training_steps=train_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4cad5f-9242-4f39-a400-2a8319b4eac1",
   "metadata": {},
   "source": [
    "## Start train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a076a622-355a-4394-b082-da895547d947",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses, val_losses =  train()\n",
    "torch.cuda.empty_cache()\n",
    "plt.plot(train_losses)\n",
    "plt.plot(val_losses)\n",
    "plt.title('model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'val'], loc='upper left')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49db06d4-9145-4fec-be9f-3bca6409e2e5",
   "metadata": {},
   "source": [
    "## Evaluate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "702066fd-bfc3-4c0f-be01-434db770f9f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = EasyQADataset(\n",
    "                           df=test_df,\n",
    "                           image_encoder = image_encoder,\n",
    "                           text_encoder = text_encoder,\n",
    "                           tokenizer = tokenizer,\n",
    "                           image_processor = image_processor\n",
    "                           )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "ac0239cd-8413-4c65-8c9a-ca8171edd193",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "EasyQAMidFusionNetwork(\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc1): Linear(in_features=768, out_features=256, bias=True)\n",
       "  (bn1): BatchNorm1d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (classifier): Linear(in_features=256, out_features=13, bias=True)\n",
       "  (relu_f): ReLU()\n",
       ")"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = \"cuda:0\"\n",
    "model.load_state_dict(torch.load('/home/jovyan/ars/vqa-research/VQA/ROBERTA_VIT_model/models/easyvqa_finetuned_epoch_4.model'))\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7263c61c-15e1-41cd-b7a0-6bfd0a8b79ec",
   "metadata": {},
   "source": [
    "### Evaluating Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 406,
   "id": "2d22765a-4468-4c17-bf5f-9b13b2c731ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_test = DataLoader(test_dataset,\n",
    "                            sampler=SequentialSampler(test_dataset),\n",
    "                            batch_size=128)\n",
    "\n",
    "_, preds, truths, confidence = evaluate(dataloader_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "f9e1864a-98aa-4819-a0bc-2ceacef63bad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3529989077494695 [0 0 0 ... 1 0 0] [1 0 0 ... 0 1 0] [0.29199582 0.27244446 0.26816538 ... 0.2706765  0.28277642 0.32675567]\n"
     ]
    }
   ],
   "source": [
    "print(_,preds,truths,confidence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "id": "2d9d7cac-20eb-4125-a60a-a1b05b18a58a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 49.06%\n"
     ]
    }
   ],
   "source": [
    "# Calculate accuracy\n",
    "def calc_accuracy(preds, truths):\n",
    "    correct_predictions = sum([1 for p, g in zip(preds, truths) if p == g])\n",
    "    total_predictions = len(preds)\n",
    "    accuracy = correct_predictions / total_predictions\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "print(f\"Accuracy: {calc_accuracy(preds, truths) * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "id": "4ab2b59d-fb84-476e-8ea9-5476bb2fb457",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_array(array):\n",
    "    result = []\n",
    "    length = len(array)\n",
    "\n",
    "    for i in range(0, length, 8):\n",
    "        sub_array = [\"no\" if value == 0 else \"yes\" for value in array[i:i+8]]\n",
    "        result.append(sub_array)\n",
    "\n",
    "    return result\n",
    "\n",
    "new_preds = convert_array(preds)\n",
    "new_truths = convert_array(truths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 366,
   "id": "5b6952db-84bd-4c2c-8854-d2174a29a9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric = evaluate.load('rouge')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "id": "85b87781-1f92-4c38-a9e9-ff90cf6f3dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(new_preds)):\n",
    "    rouge_metric.add_batch(predictions=new_preds[i], references=new_truths[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 407,
   "id": "c9698da7-69da-4d16-b72a-2f3ece6e5353",
   "metadata": {},
   "outputs": [],
   "source": [
    "rouge_metric.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da9cb0b5-fc1a-4adf-beae-0ddcdcbb5679",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
